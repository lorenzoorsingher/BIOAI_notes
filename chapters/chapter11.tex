\section{Genetic Programming}
\textbf{Genetic programming is a type of evolutionary algorithm designed to evolve computer programs automatically}. This is achieved by representing programs as tree structures and using principles of natural selection to find optimal solutions.

\subsection*{The Challenge}
\begin{itemize}
    \item Virtually all problems in computer science can be framed as a search for a computer program that solves a specific problem.
    \item  Alan Turing first proposed the idea of using evolutionary search in the program space in 1948.
   \item Arthur Samuel, an AI pioneer, asked: "How can computers learn to solve problems without being explicitly programmed?" This question underlies the field of Genetic Programming.
    \item Genetic programming provides a method for automatically creating a working computer program from a high-level problem statement, also known as automatic programming, program synthesis, or program induction.
\end{itemize}

\section{Generalities}
\begin{itemize}
    \item Genetic programming (GP) is a specialization of genetic algorithms (GAs).
    \item The primary difference between GAs and GP is that GP uses tree-based representations for individuals, whereas GAs use linear structures.
     \item The original goal of GP was to develop a general framework to evolve computer programs.
    \item  Currently, GP is mostly used for black-box (data-driven) modelling, forecasting, and classification tasks. It serves as an alternative to neural networks.
    \item Early attempts to evolve computer programs were made by Smith (1980), and the first GP implementations were by Camamer (1985) and Koza (1989) in LISP.
    \item \textbf{John Koza is considered the "founding father" of genetic programming}.
    \item GP produces human-readable models/programs, unlike neural networks, enabling inspection, modification, and debugging.
   \item GP allows the reuse of existing subprograms, facilitating scalability and hierarchical expansion.
    \item  GP's weaknesses include its need for large populations and slow convergence, although research is making progress to improve this.
\end{itemize}

\section{Why Trees?}
\begin{itemize}
    \item Trees are a universal form capable of representing various structures:
    \begin{itemize}
        \item Arithmetic expressions (e.g., $y = x * ln(a) + sin(z) / exp(-x) - 3.4$).
         \item Boolean expressions (e.g., $(x_1 \text{ AND NOT } x_2) \text{ OR } (\text{NOT } x_1 \text{ AND } x_2)$).
       \item Computer programs (including loops, if-then-else structures).
    \end{itemize}
\end{itemize}

\subsection*{Implications}
\begin{itemize}
    \item Adaptive individuals: the size of individuals is not fixed; it depends on the depth of the tree and its branching factor.
    \item Domain-specific grammar is needed to accurately reflect the problem and should represent any possible expression.
\end{itemize}

\section{Grammar Definition}
\begin{itemize}
    \item  A grammar is defined using two sets: the terminal set and the function set.
   \item \textbf{Terminal set (T)}: specifies all variables and constants, which are the leaves of the trees.
    \item \textbf{Function set (F)}: contains all functions that can be applied to the elements of the terminal set, such as arithmetic, Boolean, decision structures like if-then-else, and loops.
    \item Each function $f$ in $F$ has its own arity (number of arguments).
   \item \textbf{Closure property:} Generally, each function $f$ in $F$ can take any other function $g$ in $F$ as an argument (GP expressions are usually not typed). There is also a variant called strongly typed genetic programming which does not have this closure property.
    \item  Correct expressions generated by the grammar can be defined recursively:
    \begin{itemize}
        \item Every $t$ in $T$ is a correct expression.
        \item $f(e_1, e_2, ..., e_n)$ is a correct expression if and only if:
        \begin{enumerate}
            \item $f$ is in $F$
            \item $arity(f) = n$ is correct
            \item $e_1, e_2, ..., e_n$ are correct expressions
         \end{enumerate}
    \end{itemize}
    \item Semantic rules can be added to characterize valid expressions.
\end{itemize}

\section{Examples}
\subsection*{Boolean Expression}
\begin{itemize}
    \item  Goal: Evolve an expression such as $(x_1 \text{ AND NOT } x_2) \text{ OR } (\text{NOT } x_1 \text{ AND } x_2)$.
  \item  Function set: $\{\text{AND, OR, NOT}\}$.
   \item  Terminal set: $\{x_1, x_2\}$, where $x_1, x_2 \in \{0,1\}$.
    \item A truth table with target outputs for each combination of $x_1$ and $x_2$ is used to train the model.
\end{itemize}
\begin{center}
\begin{tabular}{ccc}
$x_1$ & $x_2$ & Target Output \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{center}

\subsection*{Symbolic Regression}
\begin{itemize}
    \item Goal: Evolve an arithmetic expression such as $y = x * ln(a) + \frac{sin(z)}{exp(-x)} - 3.4$.
    \item  Given a set of training samples in $\mathbb{R}^2$: $\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$.
   \item  Function set: $\{-, +, *, /, \text{sin, exp, ln}\}$.
    \item  Terminal set: $\{a, x, z, 3.4\}$, with $a, x, z \in \mathbb{R}$.
     \item A GP solution will find a function $f(x)$ such that $f(x_i) = y_i$ for each $i$.
\end{itemize}

\subsection*{Symbolic Regression (Black-Box)}
\begin{itemize}
 \item Goal: Find a function $f(x)$ such that for every $i$ in $\{1, 2, ..., n\}$, $f(x_i) = y_i$, given points in $\mathbb{R}^2$: $\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$.
    \item  Function set: $\{-, +, *, /, \text{sin, exp, ln}\}$.
    \item  Terminal set: $\mathbb{R} + \{x\}$.
     \item  Fitness is the sum of squared errors: $\sum_{i=1}^{n} (f(x_i) - y_i)^2$, or a similar metric such as mean squared error (MSE) or root mean squared error (RMSE).
\end{itemize}

\subsection*{Computer Program}
\begin{itemize}
    \item  Goal: Evolve a loop cycle (e.g. an example is provided that increments $i$ from 1 to 20).
    \item  Given an input and a desired program output.
  \item Example input: $i=1$; Example output: $i=20$.
    \item  Function set: $\{\text{while, <, >, =}\}$.
    \item Terminal set: $\{i, 20\}$, with $i \in \mathbb{Z}^+$.
\end{itemize}
\begin{lstlisting}[language=C]
i = 1; 
while (i < 20) {
  i = i + 1; 
}
\end{lstlisting}

\subsection*{Computer Program (Controller)}
\begin{itemize}
    \item  Goal: Evolve a controller for guiding an agent (e.g., a mouse) towards a goal (cheese) in a maze.
    \item Function set: $\{\text{If-Way-Ahead-Blocked, While-Not-At-Cheese (root node)}\}$.
    \item  Terminal set: $\{\text{Move-Forward, Turn-Left, Turn-Right}\}$.
\end{itemize}

\begin{itemize}
    \item Example of a possible evolved controller program:

\begin{lstlisting}[]
While not at the cheese
    If the way ahead is blocked
        Turn right 90 degrees
        Move forward one space
        Move forward one space
        Move forward one space
    Otherwise
        Move forward one space
        Turn right 90 degrees
        Move forward one space
        Move forward one space
        Turn left 90 degrees
            If the way ahead is blocked
                Turn left 90 degrees
            Otherwise
                 Move forward one space
\end{lstlisting}
\end{itemize}

\subsection*{Classification (Bank Credit Scoring)}
\begin{itemize}
    \item Goal: A bank wants to distinguish "good" from "bad" loan applicants.
    \item A model is needed that matches historical data:
    \begin{center}
    \begin{tabular}{ccccc}
        ID & No of Children & Salary & Marital status & Class \\
        \hline
        ID-1 & 2 & 45000 & Married & bad \\
        ID-2 & 0 & 30000 & Single & good \\
        ID-3 & 1 & 40000 & Divorced & good \\
        ... & ... & ... & ... & ...
    \end{tabular}
\end{center}
    \item  A possible model: IF $(\text{NOC} = 2)$ AND $(\text{S} > 80000)$ THEN good ELSE bad.
   \item  Function set: $\{\text{AND, OR, NOT, <, >, =}\}$.
    \item Terminal set: $\{\text{NOC, S, Married/Single/Divorced, C}\}$, with $C \in \mathbb{Z}^+$.
    \item This approach allows for interpretable models, unlike black-box methods.
    \item In general, the goal in classification problems is to identify an if-then rule of the kind: IF [expression] THEN [target output].
   \item The search space is a set of expressions (phenotypes).
   \item The fitness of an expression is the percentage of well-classified cases (supervised learning).
    \item The representation (genotypes) of expressions are parse trees, also called Symbolic-Expressions (S-expressions).
\end{itemize}

\section{Fitness Functions}
\begin{itemize}
    \item  Fitness functions are usually problem-dependent.
   \item  Fitness calculation requires evaluating the expression/program against a number of test cases to determine if it matches the desired outcome.
    \begin{itemize}
        \item For Boolean expressions: fitness is the number of correctly predicted target outputs.
        \item For arithmetic expressions: fitness is the sum of squared errors with respect to supplied target outputs.
        \item For computer programs: fitness is the number of correctly generated target outputs.
    \end{itemize}
    \item Fitness functions may include a penalty to penalize individuals with undesirable structural properties such as invalid expressions or big tree sizes.
\end{itemize}

\section{Differences Between GP and GA}
\begin{itemize}
    \item \textbf{Representation:} In GA, chromosomes are linear structures (bit strings, integer strings, real-valued vectors, permutations). In GP, chromosomes are tree-shaped, non-linear structures.
     \item \textbf{Chromosome size:} In GA, the size of the chromosomes is fixed. In GP, trees may vary in depth and width.
     \item \textbf{Use of operators:} In GA, crossover AND mutation are used. In GP, crossover OR mutation is used. Both crossover and mutation depend on user-defined probabilities.
    \item In GP, crossover is usually highly probable ($p_c > 0.8$), while mutation is quite rare ($p_m < 0.05$).
\end{itemize}

\section{Algorithmic Details}

\subsection*{Initial Population}
\begin{itemize}
    \item  Initial trees are randomly generated within a maximum depth, usually small.
    \item A root is randomly selected from the function set $F$.
    \item Two methods for choosing nodes below the root:
        \begin{enumerate}
            \item \textbf{Full method:} Each branch has depth exactly equal to $D_{max}$. Nodes at depth $d < D_{max}$ are chosen from the function set $F$ and nodes at depth $d = D_{max}$ are chosen from the terminal set $T$.
            \item \textbf{Grow method:} Each branch has a depth at most equal to $D_{max}$. Nodes at depth $d < D_{max}$ are chosen from $F \cup T$ and nodes at depth $d = D_{max}$ are chosen from $T$.
        \end{enumerate}
      \item The full and grow methods are often combined into the "ramped half-and-half method", where each method generates 50\% of the initial individuals.
\end{itemize}

\subsection*{Parent Selection}
\begin{itemize}
    \item  Classic GP: fitness-proportionate selection.
    \item  Modern GP implementations use "over-selection" to increase efficiency.
        \begin{enumerate}
            \item Rank the population by fitness.
             \item Divide into two groups:
                \begin{itemize}
                    \item Group 1: best x\% of the population.
                    \item Group 2: other (100-x)\%.
                 \end{itemize}
            \item Select 80\% of parents from group 1 and 20\% from group 2.
            \item $x\%$ thresholds are usually set by rules of thumb. For populations of 1000, 2000, 4000, and 8000, x is 32\%, 16\%, 8\% and 4\% respectively.
        \end{enumerate}
\end{itemize}

\subsection*{Survivor Selection}
\begin{itemize}
    \item  Classic GP uses very large populations (50k-600k individuals) with a purely generational scheme (no survival of parents).
   \item Modern GP uses smaller populations ($<< 10k$) with steady-state schemes and elitism to preserve the best individuals in the population.
\end{itemize}

\subsection*{Crossover}
\begin{itemize}
   \item Crossover involves selecting two parent trees, identifying two cut points (usually function nodes), and swapping the subtrees below these points to create two offspring.
   \item Crossover must check for validity in strongly typed GP.
\end{itemize}

\subsection*{Mutation}
\begin{itemize}
     \item The tree-based representation allows several possible mutations:
        \begin{itemize}
            \item Function node mutation.
            \item Terminal node mutation.
            \item Growth (expansion) mutation.
             \item Truncation mutation.
            \item Swapping mutation.
            \item Gaussian mutation.
        \end{itemize}
        \begin{itemize}
            \item \textbf{Function node mutation}: Replaces a function node with another function with the same arity.
            \item \textbf{Terminal node mutation}: Replaces a terminal node with another terminal node.
           \item \textbf{Growth mutation}: Expands the tree by adding a new randomly generated subtree at a chosen point in the existing tree.
            \item \textbf{Truncation mutation}: Shrinks the tree by removing a subtree and replacing it with a leaf.
            \item \textbf{Swapping mutation}: Swaps the order or position of subtrees within the same individual.
            \item\textbf{Gaussian mutation}: Modifies constant values by adding a Gaussian random number.
        \end{itemize}
\end{itemize}

\subsection*{Stop Conditions}
\begin{itemize}
    \item  Similar to other evolutionary algorithms:
    \begin{itemize}
        \item Number of fitness evaluations.
        \item Number of generations.
        \item Number of generations without improvement.
        \item Elapsed time.
    \end{itemize}
    \item Additional criterion for symbolic regression problems: number of "hits" reached (where "hit" is if $|f(x_i) - y_i| < \epsilon$, e.g., stop when 5000 hits with $\epsilon=0.0001$).
  \item  Multiple criteria can be combined (usually in OR).
\end{itemize}

\section{Successful Applications}
\begin{itemize}
    \item \textbf{Robotics}: automatic generation of gaits for biped robots, hand gestures for humanoid robots, mouth motions for speaking robots, and state machines for behavioral robots.
   \item \textbf{Biology}: transmembrane segment identification for proteins.
    \item \textbf{Quantum Computing}: automatic design of algorithms and communication protocols.
    \item \textbf{Electronics and TLC}: design of analog circuits (topology and RLC parameters), controllers, and antennas.
   \item\textbf{System identification} and automatic discovery of physics laws (e.g., Eureka tool).
\end{itemize}

\section{Issues in GP}

\subsection*{Bloat}
\begin{itemize}
    \item  Bloat is the tendency for GP trees to increase in size over time, which can make the models hard to handle and understand.
     \item  Possible countermeasures:
        \begin{itemize}
            \item Limit the size of trees in the initial population.
             \item Prohibit variation operators (mutation and crossover) that generate "too big" trees.
            \item Introduce checks/repair mechanisms in mutations and crossover.
            \item Pruning (worst trees replaced by branches pruned from best trees).
            \item Penalize the fitness of oversized trees (parsimony pressure).
            \item Use ad-hoc operators.
        \end{itemize}
\end{itemize}

\subsection*{Bloat ("Building Block" Approach)}
\begin{itemize}
    \item Start with simple trees (root node and first children) and let trees grow as needed during evolution.
    \item Expand trees when:
        \begin{itemize}
            \item Simplicity no longer accounts for the complexity of the problem.
           \item No improvement in fitness is observed.
        \end{itemize}
    \item Expansion occurs by adding a randomly generated building block (new node) to individuals.
     \item Expansion occurs at a specified expansion probability ($p_e$).
   \item Helps reduce the computational complexity of the evolution process and produces smaller individuals.
\end{itemize}

\subsection*{Transfer to Reality}
\begin{itemize}
    \item There is a gap between trees for data fitting and trees (programs) that are actually executable.
     \item Execution can change the environment; therefore, fitness calculation should account for that (e.g., a robot controller).
     \item Fitness calculations are mostly by simulation, which can be expensive (time-consuming).
     \item Evolved controllers are often very good despite these issues.
\end{itemize}

\subsection*{Efficiency}
\begin{itemize}
    \item  GP trees are usually very hard to evolve and require a population of thousands of individuals, with slow convergence and expensive fitness computations.
   \item  Possible solutions:
        \begin{itemize}
           \item  Parallel GP (e.g., multi-threaded GP or GP on GPU), with parallelism at the tree or instruction level.
            \item  Compiled GP (trees are precompiled with compiling optimizations before execution).
        \end{itemize}
\end{itemize}

\section{Discussion}
\subsection*{What is, in the end, GP?}
\begin{itemize}
    \item  The art of evolving computer programs.
    \item A means to "automagically" program computers.
    \item  A genetic algorithm with another representation.
\end{itemize}
