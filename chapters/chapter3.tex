This chapter explores evolutionary algorithms beyond classical genetic algorithms, focusing on modern variants. These include Evolution Strategies, Evolutionary Programming, Differential Evolution, and Estimation of Distribution Algorithms. A key concept is the co-evolution of decision variables and algorithm parameters.

\section{Evolution Strategies}
\subsection*{Generalities}
Evolution Strategies (ES) were developed by Rechenberg and Schwefel in 1973. They are real-valued algorithms primarily used for numerical optimization. Unlike genetic algorithms, each individual in ES consists of a vector $x$ of decision variables and a set $s$ of strategy parameters, such as mutation step-sizes. The algorithm aims to model the evolution of evolution by optimizing both the parameters of the problem and the algorithm itself. ES is considered solid, fast and reliable, and has a strong theoretical foundation.

\subsection*{Original vs. Modern ES}
The original ES, known as (1+1)-ES, involved one parent and one offspring, using only uncorrelated mutations without recombination. Modern ES are multi-membered, employing multiple parents, correlated mutations, and recombination. These changes have led to significant performance improvements.

\subsection*{General Framework}
The general framework involves initializing both decision variables and strategy parameters randomly. The algorithm then proceeds through typical evolutionary steps: selection of parents, crossover (if applicable) on both decision variables and strategy parameters, mutation of both, evaluation of fitness, and iterative repetition of these steps. The self-adaptation of mutation step sizes is a key difference from classical genetic algorithms, enabling the algorithm to determine the best search directions.

\subsection*{Mutation Strategies}
\subsubsection*{Static Mutation}
In static mutation, each individual generates an offspring using the formula $x_i' = x_i + \sigma \times N(0,1)$, where $\sigma$ is a fixed mutation step-size.

\subsubsection*{Adaptive Mutation}
Adaptive mutation uses the "1/5 rule" to adjust $\sigma$ every $k$ generations:
\begin{itemize}
    \item $\sigma = \sigma / c$ if $p_s > 1/5$
    \item $\sigma = \sigma \times c$ if $p_s < 1/5$
    \item $\sigma = \sigma$ if $p_s = 1/5$
\end{itemize}
Here, $p_s$ is the measured success probability of mutations, and $c$ is usually between 0.8 and 1.0. Cumulative Step-size Adaptation (CSA) adjusts $\sigma$ based on a cumulative path, combining previous step-sizes with weights that decrease with time.

\subsubsection*{Self-Adaptive Mutations}
Self-adaptive mutations encode mutation parameters into the genotype.
\paragraph*{Uncorrelated mutations with one $\sigma$} Each individual is represented as $(x_1, x_2, ..., x_n, \sigma)$, where $\sigma$ is a global step-size. Offspring are generated by:
\begin{itemize}
    \item $\sigma' = \sigma \times \exp(\tau \times N(0,1))$
    \item $x_i' = x_i + \sigma' \times N(0,1)$
\end{itemize}
where $\tau \propto 1.0 / \sqrt{n}$ is the learning rate.
\paragraph*{Uncorrelated mutations with multiple $\sigma$} Each individual is represented as $(x_1, x_2, ..., x_n, \sigma_1, \sigma_2, ..., \sigma_n)$, with each $x_i$ having an independent mutation step-size $\sigma_i$. The offspring are generated by:
\begin{itemize}
    \item $\sigma_i' = \sigma_i \times \exp(\tau' \times N(0,1) + \tau \times N(0,1))$
    \item $x_i' = x_i + \sigma_i' \times N(0,1)$
\end{itemize}
Here, $\tau' \propto 1.0 / \sqrt{2n}$ is the global learning rate, and $\tau \propto 1.0 / \sqrt{2 \sqrt{n}}$ is the coordinate-wise learning rate.
\paragraph*{Correlated mutations with multiple $\sigma$} Each individual is represented as $(x_1, ..., x_n, \sigma_1, ..., \sigma_n, \alpha_1, ..., \alpha_k)$ where $k = n(n-1)/2$. This includes $n$ independent mutation step-sizes and $k$ rotation angles that represent interactions between step-sizes for each pair of variables. An $(n \times n)$-dimensional covariance matrix $C$ is defined, with diagonal elements representing the variance $\sigma_i^2$ and off-diagonal elements representing interactions between variables. Offspring are generated by:
\begin{itemize}
    \item $\sigma_i' = \sigma_i \times \exp(\tau' \times N(0,1) + \tau \times N(0,1))$
    \item $\alpha_j' = \alpha_j + \beta \times N(0,1)$
    \item $x' = x + N(0, C')$
\end{itemize}
where $C'$ is the covariance matrix after mutation of $\sigma$ and $\alpha$ values. The covariance matrix can be obtained by $C(\sigma, \alpha) = (S \times T)^T \times (S \times T)$, where $S$ is a diagonal matrix of standard deviations, and $T$ is the product of rotation matrices $R(\alpha_{ij})$.

\subsection*{Self-Adaptation Illustration}
Self-adaptive ES can follow a moving optimum in a dynamic fitness landscape by adjusting the mutation step-size after every shift. In a static landscape, uncorrelated mutations with one $\sigma$ lead to a circular distribution of offspring, while multiple $\sigma$ result in an elliptical distribution. Correlated mutations allow the ellipsoid to rotate towards the optimal solution.

\subsection*{Selection and Recombination}
Original ES versions use comma (,) and plus (+) notation for selection strategies, such as (1+1)-ES, $(\mu, \lambda)$-ES, and $(\mu+\lambda)$-ES. Selection is based on the ranking of individuals' fitness, taking the $\mu$ best individuals (truncated selection). Modern ES use recombination, such as uniform crossover or arithmetic averaging of $\rho$ solutions, denoted as $(\mu/\rho,\lambda)$-ES or $(\mu/\rho+\lambda)$-ES, where $\rho$ is the mixing number. The value of $\rho$ determines the type of crossover; $\rho = 2$ is local crossover, while $\rho > 2$ is global crossover. Some variants introduce a parameter $\kappa$ for the maximum lifespan of an individual, denoted by $(\mu, \kappa, \lambda)$-ES.
\paragraph*{Comma vs Plus Strategies}
Comma strategies are more explorative, forgetting previous solutions, making them good for dynamic environments. Plus strategies are more exploitative and elitist, keeping the best solutions, making them good for large-scale problems or when marginal improvements are important.

\subsection*{Why Evolution Strategies Work}
ES adapts mutation step-sizes based on time and space.  In multi-membered ES, each individual has its own co-evolving mutation strategy. Different mutation step-sizes perform differently under different circumstances.

\subsection*{Covariance Matrix Adaptation Evolution Strategy (CMA-ES)}
CMA-ES is a state-of-the-art black-box optimization algorithm. It samples offspring using a multivariate Gaussian distribution. It learns from successful mutations to adapt both the step-size $\sigma$ and the covariance matrix $C$, adjusting it to fit the fitness landscape. CMA-ES learns pairwise dependencies between variables, performs a Principle Component Analysis (PCA) on mutation step-sizes, and learns a new, rotated problem representation.  It implicitly performs a PCA and learns dependencies between variables, adjusting the search to the fitness landscape.

\section{Evolutionary Programming}
\subsection*{Generalities}
Evolutionary Programming (EP), developed by Fogel et al. in 1966, is one of the oldest EAs.  Originally used for machine learning tasks with Finite State Machines (FSM), modern EP is applied to numerical optimization, often crossbred with ES. The main features of EP are deterministic reproduction, mutation only without recombination, and self-adaptation of mutation step-sizes.

\subsection*{General Framework}
The framework is similar to ES. The key steps are: initialize the population, create offspring via mutation, evaluate fitness, and use a plus selection strategy. Each individual includes both problem variables $(x_1, x_2, ..., x_n)$ and mutation step-sizes $(\sigma_1, \sigma_2, ..., \sigma_n)$, similar to uncorrelated mutations with multiple $\sigma$ in ES.

\subsection*{Algorithm Details}
\paragraph*{Representation} Each individual is represented as $(x_1, x_2, ..., x_n, \sigma_1, \sigma_2, ..., \sigma_n)$, similar to ES.
\paragraph*{Initialization} A population of $\mu$ solutions is generated with $x_i$ sampled uniformly within the decision space and $\sigma_i$ sampled uniformly within.
\paragraph*{Selection} Each solution deterministically generates one offspring, independent of fitness, unlike other evolutionary algorithms.
\paragraph*{Genetic Operators} EP uses mutation only. Each offspring is generated using:
\begin{itemize}
    \item $x_i' = x_i + \sigma_i \times N(0,1)$
    \item $\sigma_i' = \sigma_i \times (1 + \alpha \times N(0,1))$
\end{itemize}
where $\alpha$ is a parameter. The mutation step-size is updated *after* the variable, unlike in ES.
\paragraph*{Survivor Selection}
Traditional EP uses $(\mu + \mu)$ selection, where $\mu$ parents and $\mu$ offspring are combined and sorted by fitness, selecting the best $\mu$ for the next generation (elitism). Stochastic variants evaluate each solution against $q$ randomly chosen solutions. A "win" is assigned for each comparison where the solution is better, and the sum of wins (relative fitness) is used for selection.

\subsection*{Historical Applications}
\paragraph*{Evolving Finite State Machines (FSM)} FSMs have states (S), inputs (I), and outputs (O), along with a transition function ($\delta$: S x I $\rightarrow$ S x O). FSMs can be used as predictors, where the output should match the subsequent input. In the mid-1960s, Fogel used EP to evolve FSMs for tasks like predicting the next input in a sequence or predicting if a number is prime.

\paragraph*{“Blondie 24”} In 2002, Fogel evolved a checkers player using a neural network with 5046 weights and an additional weight for kings. The representation included weights and corresponding $\sigma$ values. The program played against other programs without human trainers and achieved "expert class" ranking, outperforming 99.61\% of rated players.

\section{Differential Evolution}
\subsection*{Generalities}
Differential Evolution (DE), proposed by Storn and Price in 1995, is a milestone in continuous optimization. It combines aspects of evolutionary algorithms and swarm intelligence. DE uses three parameters: population size (N), scale factor (SF), and crossover probability (CR). It is known for its ease of implementation and power.

\subsection*{Algorithm Details}
DE involves an initialization step, followed by an iterative process where for each solution in the population, mutation and crossover are applied. Selection is implemented by one-to-one spawning: the offspring replaces the parent if it is better.

\subsubsection*{Mutation}
DE uses differential vectors for mutation. A differential vector is the difference between two vectors representing solutions in the population, which is used to perturb a current solution. Common mutation strategies include:
\begin{itemize}
    \item rand/1: $x' = x + SF \times (a - b)$
    \item rand/2: $x' = x + SF \times (a - b) + SF \times (c - d)$
    \item best/1: $x' = best + SF \times (a - b)$
    \item best/2: $x' = best + SF \times (a - b) + SF \times (c - d)$
    \item cur-to-best/1: $x' = x + SF \times (best - x) + SF \times (a - b)$
    \item rand-to-best/2: $x' = a + SF \times (best - x) + SF \times (a - b) + SF \times (c - d)$
\end{itemize}
Here, $a$, $b$, $c$, and $d$ are randomly chosen, mutually exclusive individuals from the population, and $best$ is the best individual. The scale factor $SF$ typically ranges from (0,2].

\subsubsection*{Crossover}
DE uses two types of crossover: binomial ("bin") and exponential ("exp"). Binomial crossover swaps genes with a probability $CR$ across all $n$ variables and ensures that at least one gene is swapped.  Exponential crossover swaps genes until a random number exceeds $CR$, using a geometric distribution with success probability equal to $CR$.
\begin{itemize}
    \item Binomial crossover: Avg no. swapped genes = $1+CR \times n$
    \item Exponential crossover: Avg no. swapped genes = $(1-CR^n)/(1-CR)$
\end{itemize}

\subsection*{Why Differential Evolution Works}
DE's effectiveness stems from the shrinking of randomized movements during optimization. Initially, differential vectors are large due to the wide spread of solutions, enabling large search movements. As solutions converge, differential vectors become smaller, allowing for search refinements. This constitutes an implicit form of adaptation. DE is explorative at the start of optimization and exploitative towards the end.

\section{Estimation of Distribution Algorithms}
\subsection*{Generalities}
Estimation of Distribution Algorithms (EDAs) are a class of EAs that build and evolve an explicit probabilistic model of the population rather than using a set of solutions. They are also called Probabilistic Model-Building Genetic Algorithms (PMBGAs). Examples include PBIL, UMDA, EMNA, BMDA, and BOA.
\paragraph*{Note} CMA-ES is similar to an EDA, but in CMA-ES the covariance matrix models a distribution of mutation step-sizes rather than solutions.

\subsection*{Example: Population-Based Incremental Learning (PBIL)}
PBIL is suitable for binary problems. It starts with a population vector where each bit has a 0.5 probability.  Solutions are sampled from this distribution, and the best solutions are used to update the model by averaging. The probabilities in the population vector are biased towards the most promising solutions.

\section{Conclusion}
This lecture covered modern variants of evolutionary algorithms, including Evolution Strategies, Evolutionary Programming, Differential Evolution, and Estimation of Distribution Algorithms. Each algorithm offers unique approaches to optimization and adaptation, extending beyond the classical genetic algorithm paradigm.


